# vLLM + Step 3.5 支持

## 概述

本次更新为镜像添加了 vLLM 推理引擎支持，可以选择通过环境变量切换使用 llama.cpp 或 vllm 来运行模型。特别添加了对 Step 3.5 Flash 模型的支持。

## 推理引擎选择

通过 `INFERENCE_ENGINE` 环境变量选择推理引擎：

| 值 | 说明 | 默认端口 |
|---|---|---|
| `llamacpp` | 使用 llama.cpp 运行 GGUF 模型 | 8080 |
| `vllm` | 使用 vLLM 运行 HF 格式模型 | 8000 |

## 环境变量

### 推理引擎选择

```bash
INFERENCE_ENGINE=vllm  # 或 llama.cpp
```

### vLLM 参数

| 环境变量 | 默认值 | 说明 |
|---|---|---|
| `VLLM_HOST` | `0.0.0.0` | 服务监听地址 |
| `VLLM_PORT` | `8000` | 服务监听端口 |
| `VLLM_MODEL_PATH` | - | 本地模型路径 |
| `VLLM_MODEL_HF_ID` | - | HuggingFace 模型 ID |
| `VLLM_TENSOR_PARALLEL_SIZE` | `1` | Tensor 并行大小 |
| `VLLM_GPU_MEMORY_UTILIZATION` | `0.9` | GPU 显存利用率 |
| `VLLM_MAX_NUM_SEQS` | `256` | 最大序列数 |
| `VLLM_MAX_MODEL_LEN` | `32768` | 最大模型长度 |
| `VLLM_EXTRA_ARGS` | - | 额外参数 |

### 模型预设 (MODEL_PRESET)

使用 vLLM 时支持的模型预设：

| 预设值 | 说明 | 默认 HF ID |
|---|---|---|
| `step35` / `step3.5` / `step-3.5-flash` | Step 3.5 Flash | `stepfun-ai/Step-3.5-Flash` |
| `glm5` / `glm-5` | GLM-5 | - |
| `glm47flash` / `glm47` | GLM-4.7 Flash | - |
| `minimax25` | MiniMax 2.5 | - |
| `kimi25` | Kimi 2.5 | - |

## 使用示例

### 方式一：使用 Step 3.5 Flash (HuggingFace)

```bash
docker run -d \
  --gpus all \
  -p 8000:8000 \
  -p 5003:5003 \
  -p 5551:5551 \
  -v /models:/models \
  -e INFERENCE_ENGINE=vllm \
  -e MODEL_PRESET=step35 \
  -e VLLM_TENSOR_PARALLEL_SIZE=1 \
  -e HF_TOKEN=your_hf_token \
  your-image:latest
```

### 方式二：使用本地模型

```bash
docker run -d \
  --gpus all \
  -p 8000:8000 \
  -p 5003:5003 \
  -p 5551:5551 \
  -v /models:/models \
  -e INFERENCE_ENGINE=vllm \
  -e VLLM_MODEL_PATH=/models/your-model-dir \
  your-image:latest
```

### 方式三：使用 llama.cpp (默认)

```bash
docker run -d \
  --gpus all \
  -p 8080:8080 \
  -p 5003:5003 \
  -p 5551:5551 \
  -v /models:/models \
  -e INFERENCE_ENGINE=llamacpp \
  -e MODEL_PRESET=glm47flash \
  your-image:latest
```

## Docker Compose 示例

### vLLM + Step 3.5

```yaml
services:
  runnode:
    image: your-image:latest
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    ports:
      - "8000:8000"
      - "5003:5003"
      - "5551:5551"
    environment:
      INFERENCE_ENGINE: vllm
      MODEL_PRESET: step35
      VLLM_TENSOR_PARALLEL_SIZE: 1
      HF_TOKEN: ${HF_TOKEN}
    volumes:
      - /models:/models
```

### llama.cpp (默认)

```yaml
services:
  runnode:
    image: your-image:latest
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    ports:
      - "8080:8080"
      - "5003:5003"
      - "5551:5551"
    environment:
      INFERENCE_ENGINE: llama.cpp
      MODEL_PRESET: glm47flash
      MODEL_PATH_GLM47FLASH: /models/glm47.gguf
    volumes:
      - /models:/models
```

## Step 3.5 特殊参数

对于 Step 3.5 Flash 模型，推荐设置：

```bash
# Step 3.5 推荐配置
VLLM_TENSOR_PARALLEL_SIZE=1  # 单卡
VLLM_MAX_MODEL_LEN=131072     # 128K 上下文
VLLM_GPU_MEMORY_UTILIZATION=0.9
VLLM_EXTRA_ARGS=--enable-expert-parallel --reasoning-parser step3p5 --tool-call-parser step3p5
```

## 端口说明

| 端口 | 服务 |
|---|---|
| 8080 | llama.cpp API (INFERENCE_ENGINE=llamacpp) |
| 8000 | vLLM API (INFERENCE_ENGINE=vllm) |
| 5003 | OpenCode Manager |
| 5551 | OpenCode Server |
| 18789 | OpenClaw Gateway |
| 3000 | Mission Control |
| 3210 | Convex Backend |
| 3211 | Convex Site Proxy |

## 注意事项

1. vLLM 需要足够的 GPU 显存，建议 24GB 以上
2. Step 3.5 是 MoE 模型，需要启用 `--enable-expert-parallel`
3. 使用 HuggingFace 模型时需要设置 `HF_TOKEN`
4. vLLM 和 llama.cpp 不能同时运行，通过 `INFERENCE_ENGINE` 选择
