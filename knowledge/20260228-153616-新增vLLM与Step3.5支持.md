<<<<<<< HEAD
# vLLM + Step 3.5 支持

## 概述

本次更新为镜像添加了 vLLM 推理引擎支持，可以选择通过环境变量切换使用 llama.cpp 或 vllm 来运行模型。特别添加了对 Step 3.5 Flash 模型的支持。
=======
# vLLM + SGLang 多引擎支持

## 概述

本次更新为镜像添加了 vLLM 和 SGLang 推理引擎支持，可以选择通过环境变量切换使用 llama.cpp、vllm 或 sglang 来运行模型。特别添加了对 Step 3.5 Flash 和 Qwen3.5-27B-FP8 模型的支持。
>>>>>>> feat/vllm-step35-support

## 推理引擎选择

通过 `INFERENCE_ENGINE` 环境变量选择推理引擎：

| 值 | 说明 | 默认端口 |
|---|---|---|
| `llamacpp` | 使用 llama.cpp 运行 GGUF 模型 | 8080 |
| `vllm` | 使用 vLLM 运行 HF 格式模型 | 8000 |
<<<<<<< HEAD
=======
| `sglang` | 使用 SGLang 运行 HF 格式模型 | 8001 |
>>>>>>> feat/vllm-step35-support

## 环境变量

### 推理引擎选择

```bash
<<<<<<< HEAD
INFERENCE_ENGINE=vllm  # 或 llama.cpp
=======
INFERENCE_ENGINE=vllm   # 或 sglang, llama.cpp
>>>>>>> feat/vllm-step35-support
```

### vLLM 参数

| 环境变量 | 默认值 | 说明 |
|---|---|---|
| `VLLM_HOST` | `0.0.0.0` | 服务监听地址 |
| `VLLM_PORT` | `8000` | 服务监听端口 |
| `VLLM_MODEL_PATH` | - | 本地模型路径 |
| `VLLM_MODEL_HF_ID` | - | HuggingFace 模型 ID |
| `VLLM_TENSOR_PARALLEL_SIZE` | `1` | Tensor 并行大小 |
| `VLLM_GPU_MEMORY_UTILIZATION` | `0.9` | GPU 显存利用率 |
| `VLLM_MAX_NUM_SEQS` | `256` | 最大序列数 |
| `VLLM_MAX_MODEL_LEN` | `32768` | 最大模型长度 |
| `VLLM_EXTRA_ARGS` | - | 额外参数 |

<<<<<<< HEAD
### 模型预设 (MODEL_PRESET)

使用 vLLM 时支持的模型预设：

| 预设值 | 说明 | 默认 HF ID |
|---|---|---|
| `step35` / `step3.5` / `step-3.5-flash` | Step 3.5 Flash | `stepfun-ai/Step-3.5-Flash` |
| `glm5` / `glm-5` | GLM-5 | - |
| `glm47flash` / `glm47` | GLM-4.7 Flash | - |
| `minimax25` | MiniMax 2.5 | - |
| `kimi25` | Kimi 2.5 | - |

## 使用示例

### 方式一：使用 Step 3.5 Flash (HuggingFace)
=======
### SGLang 参数

| 环境变量 | 默认值 | 说明 |
|---|---|---|
| `SGLANG_HOST` | `0.0.0.0` | 服务监听地址 |
| `SGLANG_PORT` | `8001` | 服务监听端口 |
| `SGLANG_MODEL_PATH` | - | 本地模型路径 |
| `SGLANG_MODEL_HF_ID` | - | HuggingFace 模型 ID |
| `SGLANG_TP_SIZE` | `1` | Tensor 并行大小 |
| `SGLANG_MAX_MODEL_LEN` | `32768` | 最大模型长度 |
| `SGLANG_EXTRA_ARGS` | - | 额外参数 |

### 模型预设 (MODEL_PRESET)

使用 vLLM/SGLang 时支持的模型预设：

| 预设值 | 说明 | 默认 HF ID | 推荐引擎 |
|---|---|---|---|
| `step35` / `step3.5` / `step-3.5-flash` | Step 3.5 Flash | `stepfun-ai/Step-3.5-Flash` | vllm/sglang |
| `qwen35fp8` / `qwen3.5-fp8` | Qwen3.5-27B-FP8 | `Qwen/Qwen3.5-27B-FP8` | vllm/sglang |
| `glm5` / `glm-5` | GLM-5 | - | vllm/sglang |
| `glm47flash` / `glm47` | GLM-4.7 Flash | - | vllm/sglang |
| `minimax25` | MiniMax 2.5 | - | vllm/sglang |
| `kimi25` | Kimi 2.5 | - | vllm/sglang |

## 使用示例

### 方式一：使用 vLLM + Step 3.5 Flash
>>>>>>> feat/vllm-step35-support

```bash
docker run -d \
  --gpus all \
  -p 8000:8000 \
  -p 5003:5003 \
  -p 5551:5551 \
  -v /models:/models \
  -e INFERENCE_ENGINE=vllm \
  -e MODEL_PRESET=step35 \
  -e VLLM_TENSOR_PARALLEL_SIZE=1 \
  -e HF_TOKEN=your_hf_token \
  your-image:latest
```

<<<<<<< HEAD
### 方式二：使用本地模型
=======
### 方式二：使用 SGLang + Qwen3.5-27B-FP8

```bash
docker run -d \
  --gpus all \
  -p 8001:8001 \
  -p 5003:5003 \
  -p 5551:5551 \
  -v /models:/models \
  -e INFERENCE_ENGINE=sglang \
  -e MODEL_PRESET=qwen35fp8 \
  -e SGLANG_TP_SIZE=2 \
  -e HF_TOKEN=your_hf_token \
  your-image:latest
```

### 方式三：使用本地模型
>>>>>>> feat/vllm-step35-support

```bash
docker run -d \
  --gpus all \
  -p 8000:8000 \
  -p 5003:5003 \
  -p 5551:5551 \
  -v /models:/models \
  -e INFERENCE_ENGINE=vllm \
  -e VLLM_MODEL_PATH=/models/your-model-dir \
  your-image:latest
```

<<<<<<< HEAD
### 方式三：使用 llama.cpp (默认)
=======
### 方式四：使用 llama.cpp (默认)
>>>>>>> feat/vllm-step35-support

```bash
docker run -d \
  --gpus all \
  -p 8080:8080 \
  -p 5003:5003 \
  -p 5551:5551 \
  -v /models:/models \
  -e INFERENCE_ENGINE=llamacpp \
  -e MODEL_PRESET=glm47flash \
  your-image:latest
```

## Docker Compose 示例

<<<<<<< HEAD
=======
### SGLang + Qwen3.5-27B-FP8

```yaml
services:
  runnode:
    image: your-image:latest
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    ports:
      - "8001:8001"
      - "5003:5003"
      - "5551:5551"
    environment:
      INFERENCE_ENGINE: sglang
      MODEL_PRESET: qwen35fp8
      SGLANG_TP_SIZE: 2
      HF_TOKEN: ${HF_TOKEN}
    volumes:
      - /models:/models
```

>>>>>>> feat/vllm-step35-support
### vLLM + Step 3.5

```yaml
services:
  runnode:
    image: your-image:latest
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    ports:
      - "8000:8000"
      - "5003:5003"
      - "5551:5551"
    environment:
      INFERENCE_ENGINE: vllm
      MODEL_PRESET: step35
      VLLM_TENSOR_PARALLEL_SIZE: 1
      HF_TOKEN: ${HF_TOKEN}
    volumes:
      - /models:/models
```

### llama.cpp (默认)

```yaml
services:
  runnode:
    image: your-image:latest
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    ports:
      - "8080:8080"
      - "5003:5003"
      - "5551:5551"
    environment:
      INFERENCE_ENGINE: llama.cpp
      MODEL_PRESET: glm47flash
      MODEL_PATH_GLM47FLASH: /models/glm47.gguf
    volumes:
      - /models:/models
```

<<<<<<< HEAD
## Step 3.5 特殊参数

对于 Step 3.5 Flash 模型，推荐设置：

```bash
# Step 3.5 推荐配置
VLLM_TENSOR_PARALLEL_SIZE=1  # 单卡
VLLM_MAX_MODEL_LEN=131072     # 128K 上下文
VLLM_GPU_MEMORY_UTILIZATION=0.9
VLLM_EXTRA_ARGS=--enable-expert-parallel --reasoning-parser step3p5 --tool-call-parser step3p5
=======
## 模型特殊参数

### Step 3.5 Flash (vLLM/SGLang)

```bash
# vLLM 推荐配置
VLLM_TENSOR_PARALLEL_SIZE=1
VLLM_MAX_MODEL_LEN=131072
VLLM_EXTRA_ARGS=--enable-expert-parallel --reasoning-parser step3p5 --tool-call-parser step3p5

# SGLang 推荐配置
SGLANG_TP_SIZE=1
SGLANG_MAX_MODEL_LEN=131072
SGLANG_EXTRA_ARGS=--enable-expert-parallel --reasoning-parser step3p5 --tool-call-parser step3p5
```

### Qwen3.5-27B-FP8 (vLLM/SGLang)

```bash
# vLLM 推荐配置 (需要2卡)
VLLM_TENSOR_PARALLEL_SIZE=2
VLLM_MAX_MODEL_LEN=32768

# SGLang 推荐配置 (需要2卡)
SGLANG_TP_SIZE=2
SGLANG_MAX_MODEL_LEN=32768
>>>>>>> feat/vllm-step35-support
```

## 端口说明

| 端口 | 服务 |
|---|---|
| 8080 | llama.cpp API (INFERENCE_ENGINE=llamacpp) |
| 8000 | vLLM API (INFERENCE_ENGINE=vllm) |
<<<<<<< HEAD
=======
| 8001 | SGLang API (INFERENCE_ENGINE=sglang) |
>>>>>>> feat/vllm-step35-support
| 5003 | OpenCode Manager |
| 5551 | OpenCode Server |
| 18789 | OpenClaw Gateway |
| 3000 | Mission Control |
| 3210 | Convex Backend |
| 3211 | Convex Site Proxy |

## 注意事项

<<<<<<< HEAD
1. vLLM 需要足够的 GPU 显存，建议 24GB 以上
2. Step 3.5 是 MoE 模型，需要启用 `--enable-expert-parallel`
3. 使用 HuggingFace 模型时需要设置 `HF_TOKEN`
4. vLLM 和 llama.cpp 不能同时运行，通过 `INFERENCE_ENGINE` 选择
=======
1. vLLM 和 SGLang 需要足够的 GPU 显存，建议 24GB 以上
2. Qwen3.5-27B-FP8 需要至少 2 卡进行 Tensor Parallel
3. Step 3.5 是 MoE 模型，需要启用 `--enable-expert-parallel`
4. 使用 HuggingFace 模型时需要设置 `HF_TOKEN`
5. 三种引擎不能同时运行，通过 `INFERENCE_ENGINE` 选择
>>>>>>> feat/vllm-step35-support
