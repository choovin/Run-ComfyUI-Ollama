# 20260214-112039 新增llama.cpp-GLM5侧车方案并保持ComfyUI走GLM4.7

## 目标
将 GLM-5 从主 Ollama 路径拆分为 sidecar 运行：
- 主容器继续 `ComfyUI + Ollama`，默认跑 `GLM-4.7-Flash`
- sidecar 容器使用 `llama.cpp`（PR 19460）运行 `GLM-5`

## 新增与修改文件
- 新增：`sidecar/llamacpp-glm5/Dockerfile`
- 修改：`docker-compose.yml`
- 修改：`.env.example`
- 修改：`README.md`

## 方案细节
### sidecar镜像
- 基于 `nvidia/cuda` 构建 `llama-server`
- 从 `ggml-org/llama.cpp` 拉取 `pull/19460` 分支并编译
- 运行入口：`llama-server`

### compose服务
新增 `llama-cpp-glm5` 服务（profile: `glm5-sidecar`）：
- GPU: `gpus: all`
- 端口映射：`${LLAMACPP_GLM5_PORT}:8080`（默认 `18080`）
- 模型目录挂载：`./models/glm5:/models`
- 模型参数由环境变量控制（模型文件、ctx、并发、GPU层等）

### 默认策略调整
- `.env.example` 中将 `DEPLOY_GLM47_FLASH_GGUF` 设为 `true`
- 保持 `DEPLOY_GLM5_UD_IQ2_XXS=false`

## 启动方式
```bash
docker compose --profile glm5-sidecar up -d --build
```

## 备注
- GLM-5 模型文件需预先放置到：
  - `./models/glm5/GLM-5-UD-TQ1_0.gguf`
- sidecar 的 GLM-5 API 地址：
  - `http://127.0.0.1:18080`
