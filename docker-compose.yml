version: "3.9"

services:
  run-comfyui-ollama:
    image: registry.cn-shenzhen.aliyuncs.com/sailfish/runnode-run-comfyui-ollama:${IMAGE_TAG:-latest}
    container_name: run-comfyui-ollama
    restart: unless-stopped

    # Requires NVIDIA Container Toolkit on the host
    gpus: all

    ports:
      - "${COMFYUI_PORT:-8188}:8188"
      - "${CODE_SERVER_PORT:-9000}:9000"
      - "${GRADIO_PORT:-7860}:7860"
      - "${OLLAMA_PORT:-11434}:11434"
      - "${SSH_PORT:-2222}:22"

    volumes:
      - ./workspace:/workspace
      - ollama_data:/root/.ollama

    environment:
      # Authentication / access
      PASSWORD: ${PASSWORD:-}
      PUBLIC_KEY: ${PUBLIC_KEY:-}
      HF_TOKEN: ${HF_TOKEN:-}

      # ComfyUI runtime args
      COMFYUI_EXTRA_ARGUMENTS: ${COMFYUI_EXTRA_ARGUMENTS:---listen}

      # Optional preset for GLM-5 GGUF
      DEPLOY_GLM5_UD_IQ2_XXS: ${DEPLOY_GLM5_UD_IQ2_XXS:-false}

      # Ollama runtime tuning (recommended for very large GGUF models)
      OLLAMA_NUM_PARALLEL: ${OLLAMA_NUM_PARALLEL:-1}
      OLLAMA_MAX_LOADED_MODELS: ${OLLAMA_MAX_LOADED_MODELS:-1}
      OLLAMA_CONTEXT_LENGTH: ${OLLAMA_CONTEXT_LENGTH:-8192}
      OLLAMA_KEEP_ALIVE: ${OLLAMA_KEEP_ALIVE:-30m}

      # Optional model auto-pull list (startup)
      OLLAMA_MODEL1: ${OLLAMA_MODEL1:-}
      OLLAMA_MODEL2: ${OLLAMA_MODEL2:-}
      OLLAMA_MODEL3: ${OLLAMA_MODEL3:-}
      OLLAMA_MODEL4: ${OLLAMA_MODEL4:-}
      OLLAMA_MODEL5: ${OLLAMA_MODEL5:-}
      OLLAMA_MODEL6: ${OLLAMA_MODEL6:-}

volumes:
  ollama_data:
