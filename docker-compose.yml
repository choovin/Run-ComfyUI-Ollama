version: "3.9"

services:
  run-comfyui-ollama:
    image: registry.cn-shenzhen.aliyuncs.com/sailfish/runnode-run-comfyui-ollama:${IMAGE_TAG:-latest}
    container_name: run-comfyui-ollama
    restart: unless-stopped

    # Requires NVIDIA Container Toolkit on the host
    gpus: all

    ports:
      - "${COMFYUI_PORT:-8188}:8188"
      - "${CODE_SERVER_PORT:-9000}:9000"
      - "${GRADIO_PORT:-7860}:7860"
      - "${OLLAMA_PORT:-11434}:11434"
      - "${OPENCODE_PORT:-4096}:4096"
      - "${OPENCODE_MANAGER_PORT:-5003}:5003"
      - "${SSH_PORT:-2222}:22"

    volumes:
      - ./workspace:/workspace
      - ollama_data:/root/.ollama

    environment:
      # Authentication / access
      PASSWORD: ${PASSWORD:-}
      PUBLIC_KEY: ${PUBLIC_KEY:-}
      HF_TOKEN: ${HF_TOKEN:-}

      # ComfyUI runtime args
      COMFYUI_EXTRA_ARGUMENTS: ${COMFYUI_EXTRA_ARGUMENTS:---listen}

      # Optional preset for GLM-5 GGUF
      DEPLOY_GLM5_UD_IQ2_XXS: ${DEPLOY_GLM5_UD_IQ2_XXS:-false}
      DEPLOY_GLM47_FLASH_GGUF: ${DEPLOY_GLM47_FLASH_GGUF:-false}

      # OpenCode server
      OPENCODE_HOSTNAME: ${OPENCODE_HOSTNAME:-0.0.0.0}
      OPENCODE_PORT: ${OPENCODE_PORT:-4096}

      # OpenCode Manager backend
      HOST: ${OPENCODE_MANAGER_HOST:-0.0.0.0}
      PORT: ${OPENCODE_MANAGER_PORT:-5003}
      WORKSPACE_PATH: ${WORKSPACE_PATH:-/workspace}
      DATABASE_PATH: ${DATABASE_PATH:-/workspace/opencode-manager/data/opencode.db}
      AUTH_SECRET: ${AUTH_SECRET:-}
      AUTH_TRUSTED_ORIGINS: ${AUTH_TRUSTED_ORIGINS:-http://127.0.0.1:5003,http://localhost:5003}
      AUTH_SECURE_COOKIES: ${AUTH_SECURE_COOKIES:-false}

      # Ollama runtime tuning (recommended for very large GGUF models)
      OLLAMA_NUM_PARALLEL: ${OLLAMA_NUM_PARALLEL:-1}
      OLLAMA_MAX_LOADED_MODELS: ${OLLAMA_MAX_LOADED_MODELS:-1}
      OLLAMA_CONTEXT_LENGTH: ${OLLAMA_CONTEXT_LENGTH:-8192}
      OLLAMA_KEEP_ALIVE: ${OLLAMA_KEEP_ALIVE:-30m}

      # Optional model auto-pull list (startup)
      OLLAMA_MODEL1: ${OLLAMA_MODEL1:-}
      OLLAMA_MODEL2: ${OLLAMA_MODEL2:-}
      OLLAMA_MODEL3: ${OLLAMA_MODEL3:-}
      OLLAMA_MODEL4: ${OLLAMA_MODEL4:-}
      OLLAMA_MODEL5: ${OLLAMA_MODEL5:-}
      OLLAMA_MODEL6: ${OLLAMA_MODEL6:-}

  llama-cpp-glm5:
    profiles: ["glm5-sidecar"]
    build:
      context: .
      dockerfile: sidecar/llamacpp-glm5/Dockerfile
      args:
        LLAMACPP_PR: ${LLAMACPP_PR:-19460}
    image: runnode/llamacpp-glm5:pr19460
    container_name: llama-cpp-glm5
    restart: unless-stopped
    gpus: all
    ports:
      - "${LLAMACPP_GLM5_PORT:-18080}:8080"
    volumes:
      - ./models/glm5:/models
    command:
      - --host
      - 0.0.0.0
      - --port
      - "8080"
      - --model
      - /models/${LLAMACPP_GLM5_MODEL_FILE:-GLM-5-UD-TQ1_0.gguf}
      - --alias
      - ${LLAMACPP_GLM5_ALIAS:-glm5}
      - --ctx-size
      - "${LLAMACPP_GLM5_CTX_SIZE:-8192}"
      - --n-gpu-layers
      - "${LLAMACPP_GLM5_N_GPU_LAYERS:-999}"
      - --threads
      - "${LLAMACPP_GLM5_THREADS:-16}"
      - --parallel
      - "${LLAMACPP_GLM5_PARALLEL:-1}"
      - --jinja
    healthcheck:
      test: ["CMD", "bash", "-lc", "curl -sf http://127.0.0.1:8080/health > /dev/null"]
      interval: 30s
      timeout: 10s
      retries: 5

volumes:
  ollama_data:
