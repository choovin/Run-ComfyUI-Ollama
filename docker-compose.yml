# llama.cpp + OpenCode Manager + OpenClaw + Mission Control 部署配置
# 基于 Jenkins Pipeline: pipeline-llamacpp-opencode-h200-mig71g.groovy
# 服务端口:
#   - llama.cpp: 8080
#   - OpenCode Manager: 5003
#   - OpenCode Server: 5551
#   - OpenClaw Gateway: 18789
#   - Mission Control: 3000
#   - Convex Backend: 3210
#   - Convex Site Proxy: 3211
#   - Convex Dashboard: 6791

services:
  runnode-llamacpp-opencode:
    container_name: runnode-llamacpp-opencode
    restart: unless-stopped
    # Use remote image from GitHub Actions build instead of local build
    # Using the latest locally available image with OpenClaw config fixes
    image: registry.cn-shenzhen.aliyuncs.com/sailfish/runnode-llamacpp-glm47-opencode:v20260228-llamacpp-opencode-openclaw-r33-llamacpp-opencode-1.2.15-manager-v0.9.04-llamacpp-opencode-1.2.15-manager-v0.9.04
    # build:
    #   context: .
    #   dockerfile: Dockerfile
    #   args:
    #     http_proxy: ${HTTP_PROXY_URL:-}
    #     https_proxy: ${HTTP_PROXY_URL:-}
    #     HTTP_PROXY: ${HTTP_PROXY_URL:-}
    #     HTTPS_PROXY: ${HTTP_PROXY_URL:-}
    # Requires NVIDIA Container Toolkit on the host
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

    ports:
      # vLLM (when INFERENCE_ENGINE=vllm)
      - "${PORT_VLLM:-8000}:8000"
      # SGLang (when INFERENCE_ENGINE=sglang)
      - "${PORT_SGLANG:-8001}:8001"
      # llama.cpp (when INFERENCE_ENGINE=llamacpp)
      - "${PORT_LLAMA:-8080}:8080"
      # OpenCode Manager
      - "${PORT_OPENCODE_MGR:-5003}:5003"
      # OpenCode Server
      - "${PORT_OPENCODE:-5551}:5551"
      # OpenClaw Gateway
      - "${PORT_OPENCLAW_GATEWAY:-18789}:18789"
      # Mission Control
      - "${PORT_OPENCLAW_MC:-3000}:3000"
      # Convex Backend (Dashboard at /dashboard path)
      - "${PORT_CONVEX_BACKEND:-3210}:3210"
      # Convex Site Proxy (HTTP Actions)
      - "${PORT_CONVEX_SITE:-3211}:3211"

    volumes:
      # 模型挂载
      - ${MODEL_MOUNT_PATH:-/data/sailfish/vllm-ollama/ollama_data/models}:/models
      # Workspace 挂载
      - ${WORKSPACE_MOUNT_PATH:-/data/sailfish/vllm-ollama/workspace}/${HELM_RELEASE_NAME:-runnode}:/workspace
      # 挂载本地 start.sh（包含 Convex 修复）
      - ./start.sh:/start.sh:ro
      # OpenClaw 配置挂载（注释掉，让 start.sh 生成配置）
      # - ${OPENCLAW_MOUNT_PATH:-/data/sailfish/vllm-ollama/openclaw-config}/${HELM_RELEASE_NAME:-runnode}:/root/.openclaw

    environment:
      # ========== 基础环境 ==========
      TZ: ${TZ:-Asia/Shanghai}
      NODE_ENV: ${NODE_ENV:-production}

      # ========== 代理配置 ==========
      http_proxy: ${HTTP_PROXY_URL:-}
      https_proxy: ${HTTP_PROXY_URL:-}
      HTTP_PROXY: ${HTTP_PROXY_URL:-}
      HTTPS_PROXY: ${HTTP_PROXY_URL:-}
      no_proxy: ${NO_PROXY_LIST:-bytepiping-storage.bytebroad.com,bytepiping-storage.bytebroad.com.cn,localhost,127.0.0.1,::1,.svc,.svc.cluster.local,169.254.169.254}
      NO_PROXY: ${NO_PROXY_LIST:-bytepiping-storage.bytebroad.com,bytepiping-storage.bytebroad.com.cn,localhost,127.0.0.1,::1,.svc,.svc.cluster.local,169.254.169.254}

      # ========== 模型选择 ==========
      # 支持：glm5 / glm47flash / minimax25 / kimi25 / step35 / qwen35fp8
      MODEL_PRESET: ${MODEL_PRESET:-glm47flash}
      GPU_PROFILE: ${GPU_PROFILE:-35g}

      # ========== 推理引擎选择 ==========
      # 支持: llama.cpp (默认), vllm, sglang
      INFERENCE_ENGINE: ${INFERENCE_ENGINE:-llamacpp}

      # 模型路径（根据 MODEL_PRESET 选择对应的路径）
      MODEL_PATH_GLM5: ${MODEL_PATH_GLM5:-}
      MODEL_PATH_GLM47FLASH: ${MODEL_PATH_GLM47FLASH:-/models/downloads/glm47flash/GLM-4.7-Flash-Q4_K_M.gguf}
      MODEL_PATH_MINIMAX25: ${MODEL_PATH_MINIMAX25:-}
      MODEL_PATH_KIMI25: ${MODEL_PATH_KIMI25:-}
      MODEL_PATH_STEP35: ${MODEL_PATH_STEP35:-}
      MODEL_PATH_QWEN35FP8: ${MODEL_PATH_QWEN35FP8:-}

      # ========== 模型自动下载 ==========
      MODEL_AUTO_DOWNLOAD: ${MODEL_AUTO_DOWNLOAD:-true}
      AUTO_DOWNLOAD_GLM47FLASH: ${AUTO_DOWNLOAD_GLM47FLASH:-true}
      MODEL_DOWNLOAD_URL: ${MODEL_DOWNLOAD_URL:-https://modelscope.cn/models/unsloth/GLM-4.7-Flash-GGUF/resolve/master/GLM-4.7-Flash-UD-TQ1_0.gguf}
      MODEL_DOWNLOAD_URLS: ${MODEL_DOWNLOAD_URLS:-}
      MODEL_DOWNLOAD_TOKEN: ${MODEL_DOWNLOAD_TOKEN:-}
      HF_TOKEN: ${HF_TOKEN:-your_hf_token}
      GLM47FLASH_DOWNLOAD_URL: ${GLM47FLASH_DOWNLOAD_URL:-}
      GLM47FLASH_DOWNLOAD_TOKEN: ${GLM47FLASH_DOWNLOAD_TOKEN:-}

      # ========== llama.cpp 参数 ==========
      LLAMACPP_HOST: ${LLAMACPP_HOST:-0.0.0.0}
      LLAMACPP_PORT: ${LLAMACPP_PORT:-8080}
      LLAMACPP_CTX_SIZE: ${LLAMACPP_CTX_SIZE:-8192}
      LLAMACPP_N_GPU_LAYERS: ${LLAMACPP_N_GPU_LAYERS:-35}
      LLAMACPP_THREADS: ${LLAMACPP_THREADS:-8}
      LLAMACPP_PARALLEL: ${LLAMACPP_PARALLEL:-1}
      LLAMACPP_EXTRA_ARGS: ${LLAMACPP_EXTRA_ARGS:---cache-type-k q8_0 --cache-type-v q8_0}

      # ========== vLLM 参数 ==========
      VLLM_HOST: ${VLLM_HOST:-0.0.0.0}
      VLLM_PORT: ${VLLM_PORT:-8000}
      VLLM_MODEL_PATH: ${VLLM_MODEL_PATH:-}
      VLLM_MODEL_HF_ID: ${VLLM_MODEL_HF_ID:-}
      VLLM_TENSOR_PARALLEL_SIZE: ${VLLM_TENSOR_PARALLEL_SIZE:-1}
      VLLM_GPU_MEMORY_UTILIZATION: ${VLLM_GPU_MEMORY_UTILIZATION:-0.9}
      VLLM_MAX_NUM_SEQS: ${VLLM_MAX_NUM_SEQS:-256}
      VLLM_MAX_MODEL_LEN: ${VLLM_MAX_MODEL_LEN:-32768}
      VLLM_EXTRA_ARGS: ${VLLM_EXTRA_ARGS:-}

      # ========== SGLang 参数 ==========
      SGLANG_HOST: ${SGLANG_HOST:-0.0.0.0}
      SGLANG_PORT: ${SGLANG_PORT:-8001}
      SGLANG_MODEL_PATH: ${SGLANG_MODEL_PATH:-}
      SGLANG_MODEL_HF_ID: ${SGLANG_MODEL_HF_ID:-}
      SGLANG_TP_SIZE: ${SGLANG_TP_SIZE:-1}
      SGLANG_MAX_MODEL_LEN: ${SGLANG_MAX_MODEL_LEN:-32768}
      SGLANG_EXTRA_ARGS: ${SGLANG_EXTRA_ARGS:-}

      # ========== OpenCode Manager 参数 ==========
      OPENCODE_HOST: ${OPENCODE_HOST:-127.0.0.1}
      OPENCODE_SERVER_PORT: ${OPENCODE_SERVER_PORT:-5551}
      OPENCODE_MANAGER_HOST: ${OPENCODE_MANAGER_HOST:-0.0.0.0}
      OPENCODE_MANAGER_PORT: ${OPENCODE_MANAGER_PORT:-5003}
      WORKSPACE_PATH: ${WORKSPACE_PATH:-/workspace}
      DATABASE_PATH: ${DATABASE_PATH:-/workspace/opencode-manager/data/opencode.db}
      AUTH_SECRET: ${AUTH_SECRET:-your_auth_secret}
      AUTH_TRUSTED_ORIGINS: ${AUTH_TRUSTED_ORIGINS:-http://0.0.0.0:5003}
      AUTH_SECURE_COOKIES: ${AUTH_SECURE_COOKIES:-false}

      # OpenCode Manager: 健康检查和安装超时
      OPENCODE_HEALTH_TIMEOUT_MS: ${OPENCODE_HEALTH_TIMEOUT_MS:-120000}
      OPENCODE_UPGRADE_TIMEOUT_MS: ${OPENCODE_UPGRADE_TIMEOUT_MS:-600000}

      # ========== OpenClaw 参数 ==========
      OPENCLAW_GATEWAY_PORT: ${OPENCLAW_GATEWAY_PORT:-18789}
      OPENCLAW_GATEWAY_HOST: ${OPENCLAW_GATEWAY_HOST:-0.0.0.0}
      OPENCLAW_GATEWAY_MODE: ${OPENCLAW_GATEWAY_MODE:-token}
      OPENCLAW_GATEWAY_TOKEN: ${OPENCLAW_GATEWAY_TOKEN:-your-token}
      OPENCLAW_GATEWAY_PASSWORD: ${OPENCLAW_GATEWAY_PASSWORD:-sailfish020}
      MISSION_CONTROL_AUTH_TOKEN: ${MISSION_CONTROL_AUTH_TOKEN:-h7ZD0gcRspozjf7sSLeeyEtupKn8lD8MxJayV2Pf754=mission_control_token}
      OPENCLAW_MISSION_CONTROL_PORT: ${OPENCLAW_MISSION_CONTROL_PORT:-3000}
      OPENCLAW_STARTUP_TIMEOUT_SEC: ${OPENCLAW_STARTUP_TIMEOUT_SEC:-60}
      OPENCLAW_MISSION_CONTROL_STARTUP_TIMEOUT_SEC: ${OPENCLAW_MISSION_CONTROL_STARTUP_TIMEOUT_SEC:-60}
      # Vite preview allowed hosts (comma-separated, or "*" for all hosts)
      OPENCLAW_MC_ALLOWED_HOSTS: ${OPENCLAW_MC_ALLOWED_HOSTS:-*}

      # ========== 钉钉配置 ==========
      DINGTALK_CLIENT_ID: ${DINGTALK_CLIENT_ID:-ding4iqz6zneluw2gyts}
      DINGTALK_CLIENT_SECRET: ${DINGTALK_CLIENT_SECRET:-your_dingtalk_client_secret}
      DINGTALK_ALLOWED_USERS: ${DINGTALK_ALLOWED_USERS:-*}

      # ========== Convex Backend 配置 ==========
      CONVEX_BACKEND_PORT: ${CONVEX_BACKEND_PORT:-3210}
      CONVEX_SITE_PORT: ${CONVEX_SITE_PORT:-3211}
      CONVEX_DATA_DIR: ${CONVEX_DATA_DIR:-/data/convex}
      CONVEX_INSTANCE_NAME: ${CONVEX_INSTANCE_NAME:-mission-control}
      CONVEX_INSTANCE_SECRET: ${CONVEX_INSTANCE_SECRET:-1eb23bc8d083299294c50f21880f3bc8d083299294c50f21880f3bc8d0832992}
      CONVEX_SELF_HOSTED_URL: ${CONVEX_SELF_HOSTED_URL:-http://127.0.0.1:3210}
      CONVEX_SELF_HOSTED_ADMIN_KEY: ${CONVEX_SELF_HOSTED_ADMIN_KEY:-}

    healthcheck:
      test: ["CMD", "curl", "-sf", "http://127.0.0.1:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s
