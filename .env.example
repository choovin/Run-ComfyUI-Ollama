# Docker image tag in ACR
IMAGE_TAG=v1.0.0-comfyui-11022026-ollama-0.16.1

# Host ports (left side). Container internal ports are fixed.
COMFYUI_PORT=8188
CODE_SERVER_PORT=9000
GRADIO_PORT=7860
OLLAMA_PORT=11434
OPENCODE_PORT=4096
OPENCODE_MANAGER_PORT=5003
SSH_PORT=2222

# Authentication / access
PASSWORD=change-me
PUBLIC_KEY=
HF_TOKEN=

# ComfyUI startup args
COMFYUI_EXTRA_ARGUMENTS=--listen

# Enable one-click GLM-5 preset (true/false)
DEPLOY_GLM5_UD_IQ2_XXS=false
# Enable one-click GLM-4.7-Flash GGUF preset (true/false)
DEPLOY_GLM47_FLASH_GGUF=false

# OpenCode server
OPENCODE_HOSTNAME=0.0.0.0

# OpenCode Manager backend
OPENCODE_MANAGER_HOST=0.0.0.0
WORKSPACE_PATH=/workspace
DATABASE_PATH=/workspace/opencode-manager/data/opencode.db
# Leave empty to auto-generate at runtime (recommended to set fixed value in production)
AUTH_SECRET=
AUTH_TRUSTED_ORIGINS=http://127.0.0.1:5003,http://localhost:5003
AUTH_SECURE_COOKIES=false

# Ollama runtime tuning (recommended for very large GGUF models)
OLLAMA_NUM_PARALLEL=1
OLLAMA_MAX_LOADED_MODELS=1
OLLAMA_CONTEXT_LENGTH=8192
OLLAMA_KEEP_ALIVE=30m

# Optional Ollama model auto-pull list (startup)
# Example:
# OLLAMA_MODEL1=hf.co/unsloth/GLM-5-GGUF:UD-TQ1_0
# OLLAMA_MODEL2=hf.co/unsloth/GLM-4.7-Flash-GGUF:Q4_K_M
OLLAMA_MODEL1=
OLLAMA_MODEL2=
OLLAMA_MODEL3=
OLLAMA_MODEL4=
OLLAMA_MODEL5=
OLLAMA_MODEL6=
